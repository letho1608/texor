Metadata-Version: 2.4
Name: texor
Version: 0.1.0
Summary: A lightweight native AI library with optimizations from TensorFlow and PyTorch
Home-page: https://github.com/letho1608/texor
Author: letho1608
Author-email: letho16082003@gmail.com
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Science/Research
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Operating System :: OS Independent
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: numpy>=1.19.0
Requires-Dist: scipy>=1.7.0
Requires-Dist: numba>=0.56.0
Requires-Dist: matplotlib>=3.3.0
Requires-Dist: tqdm>=4.62.0
Requires-Dist: rich>=10.0.0
Requires-Dist: click>=8.0.0
Provides-Extra: dev
Requires-Dist: pytest>=6.0; extra == "dev"
Requires-Dist: pytest-cov>=2.0; extra == "dev"
Requires-Dist: flake8>=3.9.0; extra == "dev"
Requires-Dist: black>=21.5b2; extra == "dev"
Requires-Dist: isort>=5.8.0; extra == "dev"
Requires-Dist: mypy>=0.900; extra == "dev"
Provides-Extra: docs
Requires-Dist: sphinx>=4.0; extra == "docs"
Requires-Dist: sphinx-rtd-theme>=0.5.0; extra == "docs"
Requires-Dist: sphinx-autodoc-typehints>=1.12.0; extra == "docs"
Provides-Extra: gpu
Requires-Dist: cupy-cuda11x>=9.0.0; extra == "gpu"
Provides-Extra: interop
Requires-Dist: onnx>=1.12.0; extra == "interop"
Requires-Dist: onnxruntime>=1.12.0; extra == "interop"
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: license-file
Dynamic: provides-extra
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# Texor - Native Machine Learning Framework

Texor is a comprehensive native machine learning framework built from the ground up using NumPy and Numba for high-performance numerical computing. It provides a clean, intuitive API similar to modern deep learning frameworks while being completely independent of TensorFlow and PyTorch.

## Key Features

### 1. Pure Native Implementation
- Built entirely with NumPy and Numba for maximum performance
- Zero external ML framework dependencies
- Full control over computational graph and automatic differentiation
- Optimized for both CPU and GPU (via CuPy) acceleration

### 2. Core Tensor API
```python
from texor.core import Tensor

# Create tensors from various sources
x = Tensor([[1, 2], [3, 4]])  # From Python list
x = Tensor(numpy_array)       # From NumPy array

# Access data and perform operations
numpy_data = x.numpy()        # Convert to NumPy
result = x.matmul(y)         # Matrix multiplication with gradient tracking
```

### 3. Neural Network Layers
```python
from texor.nn import Sequential, Linear, Conv2D, MaxPool2D, ReLU, BatchNorm2D

model = Sequential([
    Conv2D(in_channels=1, out_channels=32, kernel_size=3),
    ReLU(),
    BatchNorm2D(32),
    MaxPool2D(kernel_size=2),
    Conv2D(in_channels=32, out_channels=64, kernel_size=3),
    ReLU(),
    MaxPool2D(kernel_size=2),
    Linear(in_features=1600, out_features=10)
])
```

### 4. Optimizers
```python
from texor.optim import SGD, Adam, RMSprop

# Create optimizer
optimizer = Adam(model.parameters(), lr=0.001)
optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)
```

### 5. Loss Functions
```python
from texor.nn import MSELoss, CrossEntropyLoss, BCELoss, L1Loss, HuberLoss

# Use loss functions with automatic gradient computation
criterion = CrossEntropyLoss()
loss = criterion(predictions, targets)  # Supports backpropagation
```

## Installation

```bash
pip install texor
```

### Dependencies
- **NumPy**: Core numerical computing
- **Numba**: Just-in-time compilation for performance
- **CuPy** (optional): GPU acceleration support

```bash
# For GPU support (optional)
pip install cupy
```

## Command Line Interface (CLI)

Texor provides a powerful CLI with intuitive features:

```bash
# View environment and setup information
texor info

# List available modules
texor list

# Search for specific modules
texor list resnet

# Check environment and dependencies
texor check
```

### CLI Features:
- **Color Output**: Messages, warnings, and errors with clear color coding
- **Progress Bars**: Visual progress for long-running tasks
- **Interactive Interface**: User-friendly command line operations
- **System Information**: Detailed environment and configuration details

## Basic Example

```python
from texor.nn import Sequential, Linear, ReLU
from texor.optim import Adam
from texor.nn import MSELoss
from texor.core import Tensor
import numpy as np

# Create native model
model = Sequential([
    Linear(input_size=784, output_size=256),
    ReLU(),
    Linear(input_size=256, output_size=10)
])

# Set up training
optimizer = Adam(model.parameters(), lr=0.001)
criterion = MSELoss()

# Create sample data
x = np.random.randn(100, 784)
y = np.random.randn(100, 10)

# Training loop
for epoch in range(10):
    # Forward pass
    predictions = model(Tensor(x))
    loss = criterion(predictions, Tensor(y))
    
    # Backward pass
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")
```

## MNIST Example
See `examples/mnist_example.py` for a complete example of training a CNN on the MNIST dataset.

## API Documentation

### Core Module
- `Tensor`: Native tensor class with automatic differentiation
- `zeros`, `ones`, `randn`: Tensor creation functions
- Native operations with gradient tracking

### Neural Network (nn) Module  
- Layers: `Linear`, `Conv2D`, `MaxPool2D`, `BatchNorm2D`
- Activations: `ReLU`, `Sigmoid`
- Loss Functions: `MSELoss`, `CrossEntropyLoss`, `BCELoss`, `L1Loss`, `HuberLoss`, `SmoothL1Loss`, `KLDivLoss`
- Model: `Sequential` - Easy-to-use API for model building

### Optimizers Module
- `SGD`: Stochastic Gradient Descent with momentum
- `Adam`: Adam optimizer with native implementation
- `RMSprop`: RMSprop optimizer

## Contributing

Contributions are welcome! Please see `CONTRIBUTING.md` for more details.

## License

MIT License - see the `LICENSE` file for details.

## Language Support

For Vietnamese documentation, please see [README_VN.md](docs/README_VN.md).
